{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### phase 6: convert data from phase 5 to fit a manually restructured draft7 schema ########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## common imports ##\n",
    "from os import path, makedirs\n",
    "import glob\n",
    "import json\n",
    "from jsonschema.validators import Draft7Validator\n",
    "from sys import exit\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## common functions ##\n",
    "def fetch_mibig_json_filepaths(dir_path):\n",
    "    \"\"\"fetch mibig json paths from a specific folder\"\"\"\n",
    "    return glob.glob(path.join(dir_path, \"BGC*.json\"))\n",
    "\n",
    "def count_props(input_dict, cur_path, result):\n",
    "    \"\"\"given a (mibig?) json, construct a list of property paths\n",
    "    along with its presence count in the json object\"\"\"\n",
    "    key_path = cur_path\n",
    "    \n",
    "    if isinstance(input_dict, dict):\n",
    "        for key in input_dict.keys():\n",
    "            result = count_props(input_dict[key], \"{}/{}\".format(key_path, key), result)\n",
    "    elif isinstance(input_dict, list):\n",
    "        key_path = \"{}[]\".format(key_path)\n",
    "        for node in input_dict:\n",
    "            result = count_props(node, \"{}\".format(key_path), result)\n",
    "\n",
    "    if not isinstance(input_dict, dict):\n",
    "        if key_path not in result:\n",
    "            result[key_path] = 0\n",
    "        result[key_path] += 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def fetch_props_new_schema(input_dict, cur_path, result):\n",
    "    \"\"\"given a (mibig?) json draft7 schema, construct a list of property paths\n",
    "    along with either required == True for each properties\"\"\"\n",
    "    key_path = cur_path\n",
    "    if (\"type\" not in input_dict) or (input_dict[\"type\"] not in [\"object\", \"array\"]):\n",
    "        key_path = \"{}\".format(cur_path) # string / etc.\n",
    "    elif input_dict[\"type\"] == \"object\":\n",
    "        for key in input_dict[\"properties\"]:\n",
    "            result = fetch_props_new_schema(input_dict[\"properties\"][key], \"{}/{}\".format(key_path, key), result)\n",
    "    elif input_dict[\"type\"] == \"array\":\n",
    "        key_path = \"{}[]\".format(cur_path)\n",
    "        result = fetch_props_new_schema(input_dict[\"items\"], \"{}\".format(key_path), result)\n",
    "    \n",
    "    if key_path not in result and \"properties\" not in input_dict:\n",
    "        result[key_path] = False # can't really use this\n",
    "    return result\n",
    "\n",
    "\n",
    "def search_and_delete(key, input_dict):\n",
    "    \"\"\"delete keys from nested dict\"\"\"\n",
    "    if isinstance(input_dict, list):\n",
    "        for i in input_dict:\n",
    "            search_and_delete(key, i)\n",
    "    elif not isinstance(input_dict, dict):\n",
    "        return\n",
    "    to_del = []\n",
    "    for k in input_dict:\n",
    "        if k == key:\n",
    "            to_del.append(k)\n",
    "        elif isinstance(input_dict[k], dict):\n",
    "            search_and_delete(key, input_dict[k])\n",
    "    for k in to_del:\n",
    "        del input_dict[k]\n",
    "\n",
    "        \n",
    "def rename_key(from_key, to_key, parent_dict):\n",
    "    \"\"\"rename key in dict\"\"\"\n",
    "    if from_key in parent_dict:\n",
    "        parent_dict[to_key] = parent_dict[from_key]\n",
    "        del parent_dict[from_key]\n",
    "\n",
    "def del_key(key, parent_dict):\n",
    "    if key in parent_dict:\n",
    "        del parent_dict[key]\n",
    "        \n",
    "import time\n",
    "def date2iso(thedate):\n",
    "    strdate = thedate.strftime(\"%Y-%m-%dT%H:%M:%S\")\n",
    "    minute = (time.localtime().tm_gmtoff / 60) % 60\n",
    "    hour = ((time.localtime().tm_gmtoff / 60) - minute) / 60\n",
    "    utcoffset = \"%.2d:%.2d\" %(hour, minute)\n",
    "    if utcoffset[0] != '-':\n",
    "        utcoffset = '+' + utcoffset\n",
    "        return strdate + utcoffset\n",
    "    \n",
    "class ToDelete():\n",
    "    \"\"\"dummy class for lazy deletion of list members\"\"\"\n",
    "    pass\n",
    "\n",
    "def lazily_deletes(input_dict):\n",
    "    \"\"\"traverse and lazily delete list/dict members\"\"\"\n",
    "    if isinstance(input_dict, list):\n",
    "        new_list = []\n",
    "        for i, node in enumerate(input_dict):\n",
    "            if not isinstance(node, ToDelete):\n",
    "                input_dict[i] = lazily_deletes(node)\n",
    "                new_list.append(node)\n",
    "        return new_list\n",
    "    elif isinstance(input_dict, dict):\n",
    "        key_to_dels = []\n",
    "        for key in input_dict:\n",
    "            if not isinstance(input_dict[key], ToDelete):\n",
    "                input_dict[key] = lazily_deletes(input_dict[key])\n",
    "            else:\n",
    "                key_to_dels.append(key)\n",
    "        for key in key_to_dels:\n",
    "            del input_dict[key]\n",
    "    return input_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_props = {}\n",
    "with open(\"../../inputs/mibig_schema_phase_6.json\") as json_file:\n",
    "    json_obj = json.load(json_file)\n",
    "    schema_props = fetch_props_new_schema(json_obj, \"\", {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_genes(data):\n",
    "    # /general_params/genes/gene[]\n",
    "    if \"genes\" in data[\"cluster\"]:\n",
    "        if \"gene\" in data[\"cluster\"][\"genes\"]:\n",
    "            data[\"cluster\"][\"genes\"][\"extra_genes\"] = []\n",
    "            data[\"cluster\"][\"genes\"][\"annotations\"] = []\n",
    "            for gene in data[\"cluster\"][\"genes\"][\"gene\"]:\n",
    "                if gene.get(\"not_in_gbk\"):\n",
    "                    if gene[\"gene_id\"] == \"No protein ID\":\n",
    "                        gene[\"gene_id\"] = gene[\"gene_name\"]\n",
    "                    extra_gene = {\n",
    "                        \"id\": gene.get(\"gene_id\"),\n",
    "                    }\n",
    "                    if extra_gene[\"id\"] == None:\n",
    "                        del extra_gene[\"id\"]\n",
    "                    if \"gene_startpos\" in gene:\n",
    "                        extra_gene[\"location\"] = {\n",
    "                            \"exons\": [{\"start\": gene[\"gene_startpos\"], \"end\": gene[\"gene_endpos\"]}],\n",
    "                            \"strand\": gene[\"strand\"]\n",
    "                        }\n",
    "                    if \"aa_seq\" in gene:\n",
    "                        extra_gene[\"translation\"] = gene[\"aa_seq\"]\n",
    "                    if len(extra_gene.keys()) > 0:\n",
    "                        data[\"cluster\"][\"genes\"][\"extra_genes\"].append(extra_gene)\n",
    "                if \"evidence_genefunction\" in gene:\n",
    "                    annotation = {\n",
    "                        \"name\": gene.get(\"gene_name\"),\n",
    "                        \"id\": gene.get(\"gene_id\"),\n",
    "                        \"functions\": [{\"category\": gene[\"gene_function\"], \"evidence\": gene[\"evidence_genefunction\"]}]                    \n",
    "                    }\n",
    "                    if annotation[\"name\"] == None:\n",
    "                        del annotation[\"name\"]\n",
    "                    if annotation[\"id\"] == None:\n",
    "                        del annotation[\"id\"]\n",
    "                    if \"gene_annotation\" in gene:\n",
    "                        annotation[\"product\"] = gene[\"gene_annotation\"]\n",
    "                    if \"tailoring\" in gene:\n",
    "                        annotation[\"tailoring\"] = [gene[\"tailoring\"]]\n",
    "                    if \"gene_pubs\" in gene:\n",
    "                        annotation[\"publications\"] = gene[\"gene_pubs\"]\n",
    "                    if \"gene_comments\" in gene:\n",
    "                        annotation[\"comments\"] = gene[\"gene_comments\"]\n",
    "                    data[\"cluster\"][\"genes\"][\"annotations\"].append(annotation)\n",
    "            del data[\"cluster\"][\"genes\"][\"gene\"]\n",
    "        if \"operon\" in data[\"cluster\"][\"genes\"]:\n",
    "            data[\"cluster\"][\"genes\"][\"operons\"] = []\n",
    "            for operon in data[\"cluster\"][\"genes\"][\"operon\"]:\n",
    "                rename_key(\"operon_genes\", \"genes\", operon)\n",
    "                rename_key(\"evidence_operon\", \"evidence\", operon)\n",
    "                operon[\"evidence\"] = [operon[\"evidence\"]]\n",
    "                data[\"cluster\"][\"genes\"][\"operons\"].append(operon)\n",
    "            del data[\"cluster\"][\"genes\"][\"operon\"]\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../preprocessed/reports/p6-compounds_ids.tsv\", \"w\") as o:\n",
    "    o.write(\"bgc_id\\tcompound\\tpubchem\\tchebi\\tchembl\\tchemspider\\n\")\n",
    "\n",
    "def transform_data_compounds(data):\n",
    "    #/general_params/compounds[]/\n",
    "    for compound in data[\"cluster\"][\"compounds\"]:\n",
    "        # database_id\n",
    "        compound[\"database_id\"] = []\n",
    "        values = \"\"\n",
    "        if \"pubchem_id\" in compound and compound[\"pubchem_id\"] > 0:\n",
    "            compound[\"database_id\"].append(\"pubchem:{}\".format(compound[\"pubchem_id\"]))\n",
    "            values += str(compound[\"pubchem_id\"])\n",
    "            del compound[\"pubchem_id\"]\n",
    "        values += \"\\t\"\n",
    "        if \"chebi_id\" in compound and compound[\"chebi_id\"] > 0:\n",
    "            compound[\"database_id\"].append(\"chebi:{}\".format(compound[\"chebi_id\"]))\n",
    "            values += str(compound[\"chebi_id\"])\n",
    "            del compound[\"chebi_id\"]\n",
    "        values += \"\\t\"\n",
    "        if \"chembl_id\" in compound and compound[\"chembl_id\"] > 0:\n",
    "            compound[\"database_id\"].append(\"chembl:{}\".format(compound[\"chembl_id\"]))\n",
    "            values += str(compound[\"chembl_id\"])\n",
    "            del compound[\"chembl_id\"]\n",
    "        values += \"\\t\"\n",
    "        if \"chemspider_id\" in compound and compound[\"chemspider_id\"] > 0:\n",
    "            compound[\"database_id\"].append(\"chemspider:{}\".format(compound[\"chemspider_id\"]))    \n",
    "            values += str(compound[\"chemspider_id\"])\n",
    "            del compound[\"chemspider_id\"]\n",
    "        with open(\"../../preprocessed/reports/p6-compounds_ids.tsv\", \"a\") as o:\n",
    "            o.write(\"{}\\t{}\\t{}\\n\".format(data[\"cluster\"][\"mibig_accession\"], compound[\"compound\"], values))\n",
    "        # mass_spec_ion_type\n",
    "        rename_key(\"mass_ion_type\", \"mass_spec_ion_type\", compound)\n",
    "        # evidence\n",
    "        rename_key(\"evidence_struct\", \"evidence\", compound)\n",
    "        # chem_acts\n",
    "        rename_key(\"chem_act\", \"chem_acts\", compound)\n",
    "        # chem_targets\n",
    "        if \"chem_target\" in compound:\n",
    "            chem_targets = compound[\"chem_target\"]\n",
    "            compound[\"chem_target\"] = []\n",
    "            for chem_target in chem_targets:\n",
    "                compound[\"chem_target\"].append({\n",
    "                    \"target\": chem_target,\n",
    "                    \"publications\": []\n",
    "                })\n",
    "            rename_key(\"chem_target\", \"chem_targets\", compound)\n",
    "        # chem_moieties\n",
    "        if \"chem_moieties\" in compound:\n",
    "            moieties = []\n",
    "            for chem_moiety in compound[\"chem_moieties\"]:\n",
    "                rename_key(\"chem_moiety\", \"moiety\", chem_moiety)\n",
    "                rename_key(\"moiety_subcluster\", \"subcluster\", chem_moiety)\n",
    "                if len(chem_moiety.get(\"moiety\", \"\")) > 0:\n",
    "                    if len(chem_moiety.get(\"subcluster\", [])) < 1:\n",
    "                        chem_moiety.pop(\"subcluster\", None)\n",
    "                    moieties.append(chem_moiety)\n",
    "            compound[\"chem_moieties\"] = moieties\n",
    "            if len(moieties) < 0:\n",
    "                del compound[\"chem_moieties\"]\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_polyketide(data):\n",
    "    # /general_params/polyketide\n",
    "    if \"polyketide\" in data[\"cluster\"]:\n",
    "        polyketide = data[\"cluster\"][\"polyketide\"]\n",
    "        if \"pk_subclass\" in polyketide:\n",
    "            polyketide[\"subclasses\"] = [polyketide[\"pk_subclass\"]]\n",
    "            del polyketide[\"pk_subclass\"]\n",
    "        if \"lin_cycl_pk\" in polyketide:\n",
    "            polyketide[\"cyclic\"] = polyketide.get(\"lin_cycl_pk\") == \"Cyclic\"\n",
    "            polyketide.pop(\"lin_cycl_pk\", None)\n",
    "        if \"pks_release_type\" in polyketide:\n",
    "            polyketide[\"release_type\"] = [polyketide[\"pks_release_type\"]]\n",
    "            del polyketide[\"pks_release_type\"]\n",
    "        synthase = {\n",
    "            \"genes\": set([])\n",
    "        }\n",
    "        if \"pks_genes\" in polyketide:\n",
    "            synthase[\"genes\"] |= set(polyketide[\"pks_genes\"])\n",
    "            del polyketide[\"pks_genes\"]\n",
    "        if \"pks_subclass\" in polyketide:\n",
    "            synthase[\"subclass\"] = polyketide[\"pks_subclass\"]\n",
    "            del polyketide[\"pks_subclass\"]\n",
    "        if \"pufa_mod_doms\" in polyketide:\n",
    "            synthase[\"pufa_modification_domains\"] = polyketide[\"pufa_mod_doms\"]\n",
    "            del polyketide[\"pufa_mod_doms\"]\n",
    "        if \"nr_iterations\" in polyketide:\n",
    "            synthase[\"iterative\"] = {\n",
    "                \"nr_iterations\": polyketide[\"nr_iterations\"],\n",
    "                \"subtype\": polyketide[\"iterative_subtype\"],\n",
    "                \"cyclization_type\": polyketide.get(\"iter_cycl_type\", \"Unknown\")\n",
    "            }\n",
    "            del polyketide[\"nr_iterations\"]\n",
    "            del polyketide[\"iterative_subtype\"]\n",
    "            polyketide.pop(\"iter_cycl_type\", None)\n",
    "        if len(polyketide.get(\"trans_at\", [])) > 0:\n",
    "            synthase[\"trans_at\"] = {\n",
    "                \"genes\": polyketide[\"trans_at\"]\n",
    "            }\n",
    "            synthase[\"genes\"] |= set(polyketide[\"trans_at\"])\n",
    "        polyketide.pop(\"trans_at\", None)\n",
    "        if \"cyclases\" in polyketide:            \n",
    "            synthase[\"genes\"] |= set(polyketide[\"cyclases\"])\n",
    "        if \"pks_thioesterase\" in polyketide:\n",
    "            te_type = \"Unknown\"\n",
    "            if polyketide.get(\"pks_te_type\") in [\"Type I\", \"Type II\"]:\n",
    "                te_type = polyketide[\"pks_te_type\"]\n",
    "            synthase[\"thioesterases\"] = [{\"gene\": gene, \"thioesterase_type\": te_type} for gene in polyketide[\"pks_thioesterase\"]]\n",
    "            synthase[\"genes\"] |= set(polyketide[\"pks_thioesterase\"])\n",
    "        polyketide.pop(\"pks_thioesterase\", None)\n",
    "        polyketide.pop(\"pks_te_type\", None)\n",
    "        if \"mod_pks_genes\" in polyketide:\n",
    "            synthase[\"modules\"] = []\n",
    "            for mod_gene in polyketide[\"mod_pks_genes\"]:\n",
    "                if \"pks_module\" in mod_gene:\n",
    "                    for mod in mod_gene[\"pks_module\"]:\n",
    "                        module = {}\n",
    "                        module[\"genes\"] = [mod_gene[\"mod_pks_gene\"]]\n",
    "                        synthase[\"genes\"] |= set(module[\"genes\"])\n",
    "                        if \"module_nr\" in mod:\n",
    "                            module[\"module_number\"] = mod[\"module_nr\"]\n",
    "                        module[\"domains\"] = mod.get(\"pks_domains\", [])\n",
    "                        module[\"at_specificities\"] = [mod.get(\"at_substr_spec\", \"Unknown\")]\n",
    "                        module[\"at_specificities\"].extend(mod.get(\"at_multiple_spec\", []))\n",
    "                        if mod.get(\"evidence_at_spec\", \"\") in [\"Sequence-based prediction\", \"Structure-based inference\", \"Feeding study\", \"Activity assay\"]:\n",
    "                            module[\"evidence\"] = mod[\"evidence_at_spec\"]\n",
    "                        module[\"kr_stereochem\"] = \"Unknown\"\n",
    "                        if mod.get(\"kr_stereochem\") in [\"Inactive\", \"L-OH\", \"D-OH\"]:\n",
    "                            module[\"kr_stereochem\"] = mod[\"kr_stereochem\"]\n",
    "                        module[\"pks_mod_doms\"] = mod[\"pks_mod_doms\"]\n",
    "                        module[\"comments\"] = mod.get(\"comments\", \"\")\n",
    "                        if \"pks_evidence_skip_iter\" in mod:\n",
    "                            module[\"non_canonical\"] = {}\n",
    "                            module[\"non_canonical\"][\"skipped\"] = mod.get(\"pks_mod_skip_iter\") == \"Skipped\"\n",
    "                            module[\"non_canonical\"][\"non_elongating\"] = mod.get(\"pks_mod_skip_iter\") == \"Non-elongating\"\n",
    "                            module[\"non_canonical\"][\"iterated\"] = mod.get(\"pks_mod_skip_iter\") == \"Iterated\"\n",
    "                            if mod.get(\"pks_evidence_skip_iter\") in [\"Sequence-based prediction\", \"Structure-based inference\", \"Activity assay\"]:\n",
    "                                module[\"non_canonical\"][\"evidence\"] = [mod[\"pks_evidence_skip_iter\"]]\n",
    "                        synthase[\"modules\"].append(module)\n",
    "            del polyketide[\"mod_pks_genes\"]\n",
    "        synthase[\"genes\"] = list(synthase[\"genes\"])\n",
    "        if len(synthase[\"genes\"]) < 1:\n",
    "            del synthase[\"genes\"]\n",
    "        if len(synthase.get(\"genes\", [])) > 0:#len(synthase.keys()) > 0:\n",
    "            polyketide[\"synthases\"] = [synthase]\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_nrp(data):\n",
    "    if \"nrp\" not in data[\"cluster\"]:\n",
    "        return\n",
    "    nrp = data[\"cluster\"][\"nrp\"]\n",
    "    \n",
    "    if \"lin_cycl_nrp\" in nrp:\n",
    "        nrp[\"cyclic\"] = nrp.get(\"lin_cycl_nrp\") == \"Cyclic\"\n",
    "        nrp.pop(\"lin_cycl_nrp\", None)\n",
    "    \n",
    "    if \"nrps_thioesterase\" in nrp:\n",
    "        te_type = \"Unknown\"\n",
    "        if nrp.get(\"nrps_te_type\") in [\"Type I\", \"Type II\"]:\n",
    "            te_type = nrp[\"nrps_te_type\"]\n",
    "        nrp[\"thioesterases\"] = [{\"gene\": gene, \"thioesterase_type\": te_type} for gene in nrp[\"nrps_thioesterase\"]]\n",
    "        nrp.pop(\"nrps_thioesterase\", None)\n",
    "    nrp.pop(\"nrps_te_type\", None)\n",
    "        \n",
    "    rename_key(\"nrps_release_type\", \"release_type\", nrp)\n",
    "    if \"release_type\" in nrp:\n",
    "        nrp[\"release_type\"] = [nrp[\"release_type\"]]\n",
    "\n",
    "    if \"nrps_genes\" in nrp:\n",
    "        for nrps in nrp[\"nrps_genes\"]:\n",
    "            rename_key(\"nrps_gene\", \"gene_id\", nrps)\n",
    "            rename_key(\"nrps_module\", \"modules\", nrps)\n",
    "            if \"modules\" in nrps:\n",
    "                for module in nrps[\"modules\"]:\n",
    "                    rename_key(\"module_nr\", \"module_number\", module)\n",
    "                    module[\"active\"] = module.get(\"nrps_mod_skip_iter\") != \"Neither\"\n",
    "                    module.pop(\"nrps_mod_skip_iter\", None)\n",
    "                    if \"a_substr_spec\" in module:\n",
    "                        module[\"a_substr_spec\"].pop(\"aa_type\", None)\n",
    "                        a_spec = {\n",
    "                        }\n",
    "                        if \"prot_adom_spec\" in module[\"a_substr_spec\"]:\n",
    "                            a_spec[\"proteinogenic\"] = [module[\"a_substr_spec\"][\"prot_adom_spec\"]]\n",
    "                        if \"nonprot_adom_spec\" in module[\"a_substr_spec\"]:\n",
    "                            a_spec[\"nonproteinogenic\"] = [module[\"a_substr_spec\"][\"nonprot_adom_spec\"]]\n",
    "                        if \"a_multiple_spec\" in module[\"a_substr_spec\"]: # put into proteinogenic, so that it will trigger validator errors\n",
    "                            if \"proteinogenic\" not in a_spec:\n",
    "                                a_spec[\"proteinogenic\"] = []\n",
    "                            a_spec[\"proteinogenic\"].extend(module[\"a_substr_spec\"].get(\"a_multiple_spec\", \"\").split(\",\"))\n",
    "                        if len(a_spec.get(\"proteinogenic\", [])) + len(a_spec.get(\"nonproteinogenic\", [])) > 0:\n",
    "                            if module[\"a_substr_spec\"].get(\"evidence_a_spec\", \"Unknown\") in [\"Sequence-based prediction\", \"Structure-based inference\", \"Feeding study\", \"Activity assay\"]:\n",
    "                                a_spec[\"evidence\"] = [module[\"a_substr_spec\"][\"evidence_a_spec\"]]\n",
    "                            a_spec[\"epimerized\"] = module[\"a_substr_spec\"].get(\"epimerized\", False)\n",
    "                            a_spec[\"aa_subcluster\"] = module[\"a_substr_spec\"].get(\"aa_subcluster\", [])\n",
    "                            module[\"a_substr_spec\"] = a_spec\n",
    "                        else:\n",
    "                            module.pop(\"a_substr_spec\", None)\n",
    "                    rename_key(\"cdom_subtype\", \"c_dom_subtype\", module)\n",
    "                    if \"nrps_mod_doms\" in module:\n",
    "                        module[\"modification_domains\"] = [module[\"nrps_mod_doms\"]]\n",
    "                        del module[\"nrps_mod_doms\"]\n",
    "                    if \"nrps_evidence_skip_iter\" in module:\n",
    "                        module[\"non_canonical\"] = {}\n",
    "                        module[\"non_canonical\"][\"skipped\"] = module.get(\"nrps_mod_skip_iter\") == \"Skipped\"\n",
    "                        module[\"non_canonical\"][\"non_elongating\"] = module.get(\"nrps_mod_skip_iter\") == \"Non-elongating\"\n",
    "                        module[\"non_canonical\"][\"iterated\"] = module.get(\"nrps_mod_skip_iter\") == \"Iterated\"\n",
    "                        if module.get(\"nrps_evidence_skip_iter\") in [\"Sequence-based prediction\", \"Structure-based inference\", \"Activity assay\"]:\n",
    "                            module[\"non_canonical\"][\"evidence\"] = [module[\"nrps_evidence_skip_iter\"]]\n",
    "                        del module[\"nrps_evidence_skip_iter\"]\n",
    "                        module.pop(\"nrps_mod_skip_iter\", None)\n",
    "\n",
    "                    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_ripp(data):\n",
    "    if \"ripp\" not in data[\"cluster\"]:\n",
    "        return\n",
    "    ripp = data[\"cluster\"][\"ripp\"]\n",
    "    \n",
    "    rename_key(\"ripp_subclass\", \"subclass\", ripp)\n",
    "    \n",
    "    if \"lin_cycl_ripp\" in ripp:\n",
    "        ripp[\"cyclic\"] = ripp.get(\"lin_cycl_ripp\") == \"Cyclic\"\n",
    "        ripp.pop(\"lin_cycl_ripp\", None)\n",
    "\n",
    "    peptidases = set([])\n",
    "\n",
    "    rename_key(\"precursor_loci\", \"precursor_genes\", ripp)\n",
    "    if \"precursor_genes\" in ripp:\n",
    "        for precursor in ripp[\"precursor_genes\"]:\n",
    "            precursor[\"gene_id\"] = \",\".join(precursor[\"gene_id\"])\n",
    "            rename_key(\"core_pept_aa\", \"core_sequence\", precursor)\n",
    "            rename_key(\"lead_pept_len\", \"leader_sequence\", precursor)\n",
    "            rename_key(\"foll_pept_len\", \"follower_sequence\", precursor)\n",
    "            rename_key(\"recogn_motif\", \"recognition_motif\", precursor)\n",
    "            peptidases |= set(precursor.get(\"peptidase\", []))\n",
    "            precursor.pop(\"peptidase\", None)\n",
    "            if \"crosslinks\" in precursor:\n",
    "                cls = []\n",
    "                for crosslink in precursor[\"crosslinks\"]:\n",
    "                    rename_key(\"AA_pos_1\", \"first_AA\", crosslink)\n",
    "                    rename_key(\"AA_pos_2\", \"second_AA\", crosslink)\n",
    "                    if len(crosslink.get(\"crosslink_type\", \"\")) < 1:\n",
    "                        continue\n",
    "                    cl = {\n",
    "                        \"crosslink_type\": crosslink[\"crosslink_type\"]\n",
    "                    }\n",
    "                    if crosslink.get(\"first_AA\", -1) >= 0:\n",
    "                        if crosslink.get(\"second_AA\", -1) >= 0:\n",
    "                            cl[\"first_AA\"] = crosslink[\"first_AA\"]\n",
    "                            cl[\"second_AA\"] = crosslink[\"second_AA\"]\n",
    "                    print(cl)\n",
    "                    cls.append(cl)\n",
    "                if (len(cls) > 0):\n",
    "                    precursor[\"crosslinks\"] = cls\n",
    "                else:\n",
    "                    del precursor[\"crosslinks\"]\n",
    "            \n",
    "    \n",
    "    ripp[\"peptidases\"] = list(peptidases)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_terpene(data):\n",
    "    if \"terpene\" not in data[\"cluster\"]:\n",
    "        return\n",
    "    terpene = data[\"cluster\"][\"terpene\"]\n",
    "    \n",
    "    rename_key(\"terpene_subclass\", \"structural_subclass\", terpene)\n",
    "    rename_key(\"terpene_c_len\", \"carbon_count_subclass\", terpene)\n",
    "    rename_key(\"prenyl_transf\", \"prenyltransferases\", terpene)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_saccharide(data):\n",
    "    if \"saccharide\" not in data[\"cluster\"]:\n",
    "        return\n",
    "    saccharide = data[\"cluster\"][\"saccharide\"]\n",
    "    \n",
    "    rename_key(\"saccharide_subclass\", \"subclass\", saccharide)\n",
    "    rename_key(\"gt_genes\", \"glycosyltransferases\", saccharide)\n",
    "    if \"glycosyltransferases\" in saccharide:\n",
    "        for gt in saccharide[\"glycosyltransferases\"]:\n",
    "            rename_key(\"gt_gene\", \"gene_id\", gt)\n",
    "            rename_key(\"gt_specificity\", \"specificity\", gt)\n",
    "            evidence = gt.get(\"evidence_gt_spec\", \"Unknown\")\n",
    "            if evidence == \"Unknown\":\n",
    "                evidence = \"Sequence-based prediction\"\n",
    "            gt[\"evidence\"] = [evidence]\n",
    "            gt.pop(\"evidence_gt_spec\", None)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_alkaloid(data):\n",
    "    if \"alkaloid\" not in data[\"cluster\"]:\n",
    "        return\n",
    "    alkaloid = data[\"cluster\"][\"alkaloid\"]\n",
    "    \n",
    "    rename_key(\"alkaloid_subclass\", \"subclass\", alkaloid)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_data_other(data):\n",
    "    if \"other\" not in data[\"cluster\"]:\n",
    "        return\n",
    "    other = data[\"cluster\"][\"other\"]\n",
    "    \n",
    "    rename_key(\"other_subclass\", \"subclass\", other)\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_none_or_unknown(data):\n",
    "    if isinstance(data, dict):\n",
    "        transformed = {}        \n",
    "        for key in data:\n",
    "            if data[key] not in [\"None\", \"\"]:\n",
    "                transformed[key] = fix_none_or_unknown(data[key])\n",
    "        return transformed\n",
    "    elif isinstance(data, list):\n",
    "        transformed = []\n",
    "        for val in data:\n",
    "            if val not in [\"None\", \"Unknown\", \"\"]:\n",
    "                transformed.append(fix_none_or_unknown(val))\n",
    "        return transformed\n",
    "    else:\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../outputs/submitter.tsv\", \"w\") as o:\n",
    "    o.write(\"bgc_id\\tsubmitter_name\\tsubmitter_institution\\tsubmitter_email\\n\")\n",
    "\n",
    "def transform_data(data):\n",
    "    \n",
    "    #/changelogs/*\n",
    "    rename_key(\"changelogs\", \"changelog\", data)\n",
    "    for log in data[\"changelog\"]:\n",
    "        log[\"comments\"] = []\n",
    "        for comment in log[\"comment\"].split(\";\"):\n",
    "            comment = comment.strip()\n",
    "            if comment == \"Submitted\" and int(data[\"general_params\"][\"mibig_accession\"][3:]) < 1831:\n",
    "                comment = \"Migrated from v1.4\"\n",
    "            log[\"comments\"].append(comment)\n",
    "        log.pop(\"comment\", None)\n",
    "    \n",
    "    #/general_params/*\n",
    "    rename_key(\"general_params\", \"cluster\", data)\n",
    "    rename_key(\"complete\", \"completeness\", data[\"cluster\"][\"loci\"])\n",
    "    data[\"cluster\"][\"loci\"][\"evidence\"] = []\n",
    "    for evidence in data[\"cluster\"][\"loci\"].get(\"conn_comp_cluster\", []):\n",
    "        if evidence in [\"Sequence-based prediction\",\n",
    "                        \"Gene expression correlated with compound production\",\n",
    "                        \"Knock-out studies\",\n",
    "                        \"Enzymatic assays\",\n",
    "                        \"Heterologous expression\"]:\n",
    "            data[\"cluster\"][\"loci\"][\"evidence\"].append(evidence)\n",
    "    data[\"cluster\"][\"loci\"].pop(\"conn_comp_cluster\", None)\n",
    "    data[\"cluster\"][\"minimal\"] = data[\"cluster\"].get(\"minimal\", False)\n",
    "    if data[\"cluster\"][\"loci\"].get(\"start_coord\", 1) < 0 and data[\"cluster\"][\"loci\"].get(\"end_coord\", 1) < 0:\n",
    "        data[\"cluster\"][\"loci\"].pop(\"start_coord\", None)\n",
    "        data[\"cluster\"][\"loci\"].pop(\"end_coord\", None)\n",
    "\n",
    "    # remove submitter information, store it in a separate text file\n",
    "    with open(\"../../outputs/submitter.tsv\", \"a\") as o:\n",
    "        personal = data.get(\"personal\", {\"submitter_name\":\"MIBiG\", \"submitter_institution\":\"\", \"submitter_email\":\"mibig@secondarymetabolites.org\"})\n",
    "        o.write(\"{}\\t{}\\t{}\\t{}\\n\".format(data[\"cluster\"][\"mibig_accession\"], personal[\"submitter_name\"], personal[\"submitter_institution\"], personal[\"submitter_email\"]))\n",
    "    data.pop(\"personal\", None)\n",
    "\n",
    "    transform_data_genes(data)\n",
    "    transform_data_compounds(data)\n",
    "    \n",
    "    transform_data_polyketide(data)\n",
    "    transform_data_saccharide(data)\n",
    "    transform_data_nrp(data)\n",
    "    transform_data_ripp(data)\n",
    "    transform_data_terpene(data)\n",
    "    transform_data_alkaloid(data)\n",
    "    transform_data_other(data)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_attributes_to_schema(data):\n",
    "    transform_data(data)\n",
    "    this_file_props = count_props(data, \"\", {})\n",
    "    for prop in this_file_props:\n",
    "        if prop not in schema_props.keys():\n",
    "            print(prop)\n",
    "            sys.exit(0)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not path.exists(\"../../preprocessed/p6-json/\"):\n",
    "    makedirs(\"../../preprocessed/p6-json/\")\n",
    "    \n",
    "for json_path in sorted(fetch_mibig_json_filepaths(\"../../preprocessed/p5-json/\")):\n",
    "    json_obj = None\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        json_obj = json.load(json_file)\n",
    "        print(json_path)\n",
    "        json_obj = fix_none_or_unknown(json_obj)\n",
    "        match_attributes_to_schema(json_obj)\n",
    "        with open(path.join(\"../../preprocessed/p6-json/\", path.basename(json_path)), \"w\") as json_file:\n",
    "            json.dump(json_obj, json_file, indent=4, separators=(',', ': '), sort_keys=True)\n",
    "\n",
    "print(\"All data transformed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_error = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_errors(data, error):\n",
    "    if len(error.path) < 1:\n",
    "        # problem is in root, need a separate approach\n",
    "        print(error.path)\n",
    "    else:        \n",
    "        # get problematic parent instance from data (so that we can fix it)\n",
    "        error_container = data\n",
    "        error_container_parent = None # for catching grandparent\n",
    "        error_container_attribute = None\n",
    "        while len(error.path) > 1:\n",
    "            if len(error.path) == 2:\n",
    "                error_container_parent = error_container\n",
    "            error_container_attribute = error.path.popleft()\n",
    "            error_container = error_container[error_container_attribute] # parent node containing the error instance\n",
    "        error_attribute = error.path.popleft() # attribute from parent node containing the error instance\n",
    "\n",
    "        if isinstance(error_container, ToDelete):\n",
    "            return\n",
    "        elif isinstance(error_container[error_attribute], ToDelete):\n",
    "            return\n",
    "        \n",
    "        if error.validator == \"type\":\n",
    "            if error.validator_value == \"integer\":\n",
    "                try:\n",
    "                    error_container[error_attribute] = int(error.instance)\n",
    "                    return\n",
    "                except:\n",
    "                    pass\n",
    "            elif error.validator_value == \"number\":\n",
    "                try:\n",
    "                    error_container[error_attribute] = float(error.instance)\n",
    "                    return\n",
    "                except:\n",
    "                    pass\n",
    "            elif error.validator_value == \"boolean\":\n",
    "                if error_container[error_attribute] == \"true\":\n",
    "                    error_container[error_attribute] = True\n",
    "                elif error_container[error_attribute] == \"false\":\n",
    "                    error_container[error_attribute] = False\n",
    "            else:\n",
    "                print(error.message)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "retired = {}\n",
    "structures_14 = {}\n",
    "\n",
    "with open(\"../../inputs/mibig_schema_phase_6.json\") as json_file:\n",
    "    schema_obj = json.load(json_file)\n",
    "    validator = Draft7Validator(schema_obj)\n",
    "    errors = {}\n",
    "    for json_path in sorted(fetch_mibig_json_filepaths(\"../../preprocessed/p6-json/\")):\n",
    "        bgc_id = path.basename(json_path)\n",
    "        id_int = int(bgc_id[3:-5])\n",
    "        if (id_int < last_error) and True:\n",
    "            continue\n",
    "        with open(json_path, \"r\") as data_file:\n",
    "            data = json.load(data_file)\n",
    "            structures_14[bgc_id] = []\n",
    "            for compound in data[\"cluster\"][\"compounds\"]:\n",
    "                structures_14[bgc_id].append((compound[\"compound\"], compound.get(\"chem_struct\", \"\"), \";\".join(compound.get(\"database_id\", []))))\n",
    "            error_counts_before = 0\n",
    "            error_counts_after = 0\n",
    "            for error in sorted(validator.iter_errors(data), key=str):\n",
    "                fix_errors(data, error)\n",
    "                error_counts_before += 1\n",
    "            error_counts = 0\n",
    "            validator = Draft7Validator(schema_obj)\n",
    "            for error in sorted(validator.iter_errors(data), key=str):\n",
    "                if error.message == \"'ncbi_tax_id' is a required property\":\n",
    "                    # will be taken care of in phase 7\n",
    "                    continue\n",
    "                elif error.message == \"'organism_name' is a required property\":\n",
    "                    # will be taken care of in phase 7\n",
    "                    continue\n",
    "                elif error.path[-2] == \"evidence\" or error.path[-1] == \"evidence\": # retire this bgc, put in a list\n",
    "                    if error.path[-3] == \"loci\" or error.path[-2] == \"loci\":\n",
    "                        if bgc_id not in retired:\n",
    "                            retired[bgc_id] = set()\n",
    "                        retired[bgc_id].add(\"loci.evidence\")\n",
    "                        continue\n",
    "                    elif error.path[1] == \"genes\":\n",
    "                        # will be taken care of in phase 7/8\n",
    "                        continue\n",
    "                elif error.path[-1] == \"nr_iterations\": # retire this bgc, put in a list\n",
    "                    if bgc_id not in retired:\n",
    "                        retired[bgc_id] = set()\n",
    "                    retired[bgc_id].add(\"nr_iterations\")\n",
    "                    continue\n",
    "                elif error.path[-1] == \"module_number\": # retire this bgc, put in a list\n",
    "                    if bgc_id not in retired:\n",
    "                        retired[bgc_id] = set()\n",
    "                    retired[bgc_id].add(\"module_number\")\n",
    "                    continue\n",
    "                elif error.path[-2] == \"proteinogenic\": # retire this bgc, put in a list\n",
    "                    if bgc_id not in retired:\n",
    "                        retired[bgc_id] = set()\n",
    "                    retired[bgc_id].add(\"a_substr_spec.proteinogenic\")\n",
    "                    continue\n",
    "                elif error.path[-1] in [\"leader_sequence\", \"follower_sequence\"]: # retire this bgc, put in a list\n",
    "                    if bgc_id not in retired:\n",
    "                        retired[bgc_id] = set()\n",
    "                    retired[bgc_id].add(\"ripp.leader/follower_sequence\")\n",
    "                    continue\n",
    "                elif error.path[-1] == \"subcluster\" and error.path[-3] == \"chem_moieties\":\n",
    "                    if bgc_id not in retired:\n",
    "                        retired[bgc_id] = set()\n",
    "                    retired[bgc_id].add(\"chem_moieties.subcluster\")\n",
    "                    continue\n",
    "                error_counts_after += 1\n",
    "            print(\"Validated and fixed {}... Before {} error(s), After: {} error(s)\".format(bgc_id, error_counts_before, error_counts_after))\n",
    "            if error_counts_after > 0:\n",
    "                last_error = id_int\n",
    "                exit(1)\n",
    "            with open(path.join(\"../../preprocessed/p6-json/\", bgc_id), \"w\") as jo:\n",
    "                json.dump(data, jo, indent=4, separators=(',', ': '), sort_keys=True)\n",
    "    print(\"All data validated!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../preprocessed/reports/p6-retired_list.tsv\", \"w\") as o:\n",
    "    for bgc_id in retired:\n",
    "        o.write(\"{}\\t{}\\n\".format(bgc_id.split(\".\")[0], \";\".join(list(retired[bgc_id]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../outputs/bgc_structures_14.tsv\", \"w\") as o:\n",
    "    for bgc_id in structures_14:\n",
    "        for compound in structures_14[bgc_id]:\n",
    "            o.write(\"{}\\t{}\\t{}\\t{}\\n\".format(bgc_id[:-5], compound[0], compound[1], compound[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
