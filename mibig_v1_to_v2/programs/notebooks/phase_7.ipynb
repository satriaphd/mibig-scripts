{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### phase 7: re-check gene annotations, comparing it to the GBK, filter out ones without any new information added ######\n",
    "#### it also populate organism and taxon properties from mibig finalgbk ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import path, makedirs\n",
    "import glob\n",
    "import json\n",
    "from jsonschema import validate, Draft7Validator\n",
    "from tempfile import TemporaryDirectory\n",
    "from Bio import SeqIO, Entrez\n",
    "from Bio.Seq import Seq\n",
    "from Bio.SeqFeature import SeqFeature, FeatureLocation\n",
    "from Bio.Alphabet import generic_protein\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "import sys\n",
    "import datetime\n",
    "import time\n",
    "import re\n",
    "import urllib3\n",
    "from urllib.parse import quote\n",
    "import certifi\n",
    "from xml.dom import minidom\n",
    "http = urllib3.PoolManager(cert_reqs='CERT_REQUIRED', ca_certs=certifi.where())\n",
    "from jsonschema.validators import Draft7Validator\n",
    "from shutil import copy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ncbi_tax_ids(organism_name, email):\n",
    "    tax_ids = []\n",
    "    \n",
    "    Entrez.email = email\n",
    "    num_try = 1\n",
    "    while num_try < 6:\n",
    "        try:\n",
    "            organism_name_escaped = \"\\\"{}\\\"\".format(organism_name)\n",
    "            dom = minidom.parse(Entrez.esearch(db=\"taxonomy\", term=organism_name_escaped))\n",
    "            ids = dom.getElementsByTagName('Id')\n",
    "            if len(ids) < 1:\n",
    "                raise Exception()\n",
    "            for tax_id in ids:\n",
    "                tax_ids.append(tax_id.firstChild.nodeValue)\n",
    "            break\n",
    "        except:\n",
    "            pass\n",
    "        num_try += 1\n",
    "        time.sleep(5)\n",
    "    \n",
    "    return tax_ids\n",
    "\n",
    "def fetch_gbk(nucl_acc, email, clean_cache = False):\n",
    "    cache_folder = \"../../preprocessed/cache/cached_gbks/\"\n",
    "    cache_path = path.join(cache_folder, \"{}.gbk\".format(nucl_acc))\n",
    "\n",
    "    if not path.exists(cache_folder):\n",
    "        makedirs(cache_folder)\n",
    "    \n",
    "    if clean_cache or not path.exists(cache_path) or path.getsize(cache_path) < 100:\n",
    "        if nucl_acc.startswith(\"MIBIG.BGC\"):\n",
    "            [_, bgc_id, cl_id] = nucl_acc.split(\".\")\n",
    "            mibig_final_file_path = \"../../inputs/cached_mibig_finalgbks/{}.1.final.gbk\".format(bgc_id)\n",
    "            with open(mibig_final_file_path, \"r\") as mibig_final_file:\n",
    "                with open(cache_path, \"w\", encoding=\"utf-8\") as cache_gbk_file:\n",
    "                    write_to_file = False\n",
    "                    for line in mibig_final_file:\n",
    "                        if line.startswith(\"LOCUS\"):\n",
    "                            gbk_accession = line.split(\" \")[1]\n",
    "                            if gbk_accession == \"{}.{}\".format(bgc_id, cl_id):\n",
    "                                write_to_file = True\n",
    "                        if write_to_file:\n",
    "                            cache_gbk_file.write(line)\n",
    "                        if line.rstrip() == \"//\":\n",
    "                            break\n",
    "        else: # (re)download from ncbi\n",
    "            Entrez.email = email\n",
    "            handle = Entrez.efetch(db=\"nucleotide\", id=nucl_acc, rettype=\"gbwithparts\", retmode=\"text\")\n",
    "            if not path.exists(cache_folder):\n",
    "                makedirs(cache_folder)\n",
    "            with open(cache_path, \"w\") as gbk_file:\n",
    "                gbk_file.write(handle.read())\n",
    "            \n",
    "    return open(cache_path, \"r\")\n",
    "\n",
    "# from antismash\n",
    "def get_aa_translation(seq_record, feature):\n",
    "    \"\"\"Obtain content for translation qualifier for specific CDS feature in sequence record\"\"\"\n",
    "    extracted = feature.extract(seq_record.seq).ungap('-')\n",
    "\n",
    "    # ensure the extracted section is a multiple of three by trimming any excess\n",
    "    if len(extracted) % 3 != 0:\n",
    "        extracted = extracted[:-(len(extracted) % 3)]\n",
    "\n",
    "    fasta_seq = extracted.translate(to_stop=True)\n",
    "    if len(fasta_seq) == 0:\n",
    "        print(\"Retranslating {} with stop codons\".format(feature.id))\n",
    "        fasta_seq = extracted.translate()\n",
    "\n",
    "    # replace ambiguous aminos with an explicit unknown\n",
    "    string_version = str(fasta_seq)\n",
    "    for bad in \"*BJOUZ\":\n",
    "        string_version = string_version.replace(bad, \"X\")\n",
    "\n",
    "    # and remove any gaps\n",
    "    string_version = string_version.replace(\"-\", \"\")\n",
    "    fasta_seq = Seq(string_version, generic_protein)\n",
    "\n",
    "    return fasta_seq\n",
    "\n",
    "def get_aa_sequence(feature, to_stop=False):\n",
    "    \"\"\"Extract sequence from specific CDS feature in sequence record\"\"\"\n",
    "    fasta_seq = feature.qualifiers['translation'][0]\n",
    "    if \"*\" in fasta_seq:\n",
    "        if to_stop:\n",
    "            fasta_seq = fasta_seq.split('*')[0]\n",
    "        else:\n",
    "            fasta_seq = fasta_seq.replace(\"*\",\"X\")\n",
    "    if \"-\" in fasta_seq:\n",
    "        fasta_seq = fasta_seq.replace(\"-\",\"\")\n",
    "    return fasta_seq\n",
    "\n",
    "\n",
    "def count_props(input_dict, cur_path, result):\n",
    "    \"\"\"given a (mibig?) json, construct a list of property paths\n",
    "    along with its presence count in the json object\"\"\"\n",
    "    key_path = cur_path\n",
    "    \n",
    "    if isinstance(input_dict, dict):\n",
    "        for key in input_dict.keys():\n",
    "            result = count_props(input_dict[key], \"{}/{}\".format(key_path, key), result)\n",
    "    elif isinstance(input_dict, list):\n",
    "        key_path = \"{}[]\".format(key_path)\n",
    "        for node in input_dict:\n",
    "            result = count_props(node, \"{}\".format(key_path), result)\n",
    "\n",
    "    if not isinstance(input_dict, dict):\n",
    "        if key_path not in result:\n",
    "            result[key_path] = 0\n",
    "        result[key_path] += 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def fetch_props_new_schema(input_dict, cur_path, result):\n",
    "    \"\"\"given a (mibig?) json draft7 schema, construct a list of property paths\n",
    "    along with either required == True for each properties\"\"\"\n",
    "    key_path = cur_path\n",
    "    if (\"type\" not in input_dict) or (input_dict[\"type\"] not in [\"object\", \"array\"]):\n",
    "        key_path = \"{}\".format(cur_path) # string / etc.\n",
    "    elif input_dict[\"type\"] == \"object\":\n",
    "        for key in input_dict[\"properties\"]:\n",
    "            result = fetch_props_new_schema(input_dict[\"properties\"][key], \"{}/{}\".format(key_path, key), result)\n",
    "    elif input_dict[\"type\"] == \"array\":\n",
    "        key_path = \"{}[]\".format(cur_path)\n",
    "        result = fetch_props_new_schema(input_dict[\"items\"], \"{}\".format(key_path), result)\n",
    "    \n",
    "    if key_path not in result and \"properties\" not in input_dict:\n",
    "        result[key_path] = False # can't really use this\n",
    "    return result\n",
    "\n",
    "def sanitise_gene_name(name):\n",
    "    if name is None:\n",
    "        return None\n",
    "    name = str(name)\n",
    "    illegal_chars = set(\"!\\\"#$%&()*+,:; \\r\\n\\t=>?@[]^`'{|}/ \")\n",
    "    for char in set(name).intersection(illegal_chars):\n",
    "        name = name.replace(char, \"_\")\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_mibig_final_gbk(bgc_id, clean_cache = False):\n",
    "    return open(\"../../inputs/cached_mibig_finalgbks/{}.1.final.gbk\".format(bgc_id), \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _fetch_val_to_ref(input_dict, parent_dict, key, all_references):\n",
    "    if isinstance(input_dict, dict):\n",
    "        for key in input_dict.keys():\n",
    "            _fetch_val_to_ref(input_dict[key], input_dict, key, all_references)\n",
    "    elif isinstance(input_dict, list):\n",
    "        for i, node in enumerate(input_dict):\n",
    "            _fetch_val_to_ref(node, input_dict, i, all_references)\n",
    "    elif isinstance(input_dict, str):\n",
    "        if parent_dict != None:\n",
    "            valu = input_dict.upper()\n",
    "            if valu not in all_references:\n",
    "                all_references[valu] = []\n",
    "            all_references[valu].append((parent_dict, key))\n",
    "    return all_references\n",
    "\n",
    "with open(\"../../preprocessed/reports/p7-updated_cases.tsv\", \"w\") as uc:\n",
    "    uc.write(\"\")\n",
    "    \n",
    "def _make_gene_id_match_reference(input_dict, reference_ids, bgc_id):\n",
    "    all_references = _fetch_val_to_ref(input_dict, None, None, {})\n",
    "    all_rids = {}\n",
    "    for rid in reference_ids:\n",
    "        if len(rid) > 0:\n",
    "            all_rids[rid.upper()] = rid\n",
    "    matches = set(all_references.keys()).intersection(set(all_rids.keys()))    \n",
    "    for key in matches:\n",
    "        for ar in all_references[key]:\n",
    "            if ar[0][ar[1]] != all_rids[key]:\n",
    "                with open(\"../../preprocessed/reports/p7-updated_cases.tsv\", \"a\") as uc:\n",
    "                    uc.write(\"{}\\t{}\\t{}\\n\".format(bgc_id, ar[0][ar[1]], all_rids[key]))\n",
    "                ar[0][ar[1]] = all_rids[key]\n",
    "\n",
    "def make_gene_id_match_reference(data, reference_ids):\n",
    "    bgc_id = data[\"cluster\"][\"mibig_accession\"]\n",
    "    _make_gene_id_match_reference(data[\"cluster\"][\"compounds\"], reference_ids, bgc_id)\n",
    "    _make_gene_id_match_reference(data[\"cluster\"].get(\"genes\", {}), reference_ids, bgc_id)\n",
    "    _make_gene_id_match_reference(data[\"cluster\"].get(\"polyketide\", {}), reference_ids, bgc_id)\n",
    "    _make_gene_id_match_reference(data[\"cluster\"].get(\"nrp\", {}), reference_ids, bgc_id)\n",
    "    _make_gene_id_match_reference(data[\"cluster\"].get(\"ripp\", {}), reference_ids, bgc_id)\n",
    "    _make_gene_id_match_reference(data[\"cluster\"].get(\"terpene\", {}), reference_ids, bgc_id)\n",
    "    _make_gene_id_match_reference(data[\"cluster\"].get(\"saccharide\", {}), reference_ids, bgc_id)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "removed_extra_genes = {}\n",
    "removed_annotations = {}\n",
    "removed_operons = {}\n",
    "no_taxid = {}\n",
    "added_mibig_genes = {}\n",
    "no_gbk = {}\n",
    "fixed = {}\n",
    "ripps_to_fix = {}\n",
    "\n",
    "nt_length_and_cds_count = {}\n",
    "\n",
    "def check_gene_annotations(data):\n",
    "    loci = data[\"cluster\"][\"loci\"]\n",
    "    annots = data[\"cluster\"].get(\"genes\", {})\n",
    "    gbk_acc = loci[\"accession\"].upper()\n",
    "    bgc_id = data[\"cluster\"][\"mibig_accession\"]\n",
    "    email = \"mibig@secondarymetabolites.org\"\n",
    "    gbk_record = None\n",
    "    if True:\n",
    "        num_try = 1\n",
    "        clean_cache = False\n",
    "        while num_try < 6:\n",
    "            try:\n",
    "                with fetch_gbk(gbk_acc, email, clean_cache) as gbk_handle:\n",
    "                    seq_record = SeqIO.read(gbk_handle, \"genbank\") # the gbk should contains only 1 file\n",
    "                    if len(seq_record.seq) < 1:\n",
    "                        raise Exception(\"Empty sequence record {}\".format(gbk_acc))\n",
    "                    gbk_record = seq_record\n",
    "                    break\n",
    "            except:\n",
    "                print(\"Error...\")\n",
    "                clean_cache = True\n",
    "            num_try += 1\n",
    "            time.sleep(5)\n",
    "        if not isinstance(gbk_record, SeqRecord): # failed to download NCBI data\n",
    "            print(\"{} Failed to download: {}\".format(data[\"cluster\"][\"mibig_accession\"], gbk_acc))\n",
    "            no_gbk[bgc_id] = gbk_acc\n",
    "            return {}\n",
    "\n",
    "    # fill up taxid\n",
    "    tax_ids = get_ncbi_tax_ids(gbk_record.annotations[\"organism\"], email)\n",
    "    if len(tax_ids) != 1:\n",
    "        no_taxid[bgc_id] = gbk_record.annotations[\"organism\"]\n",
    "        print(tax_ids)\n",
    "    else:\n",
    "        data[\"cluster\"][\"ncbi_tax_id\"] = tax_ids[0]\n",
    "        data[\"cluster\"][\"organism_name\"] = gbk_record.annotations[\"organism\"]\n",
    "         \n",
    "    # fetch all CDS inside the cluster\n",
    "    cluster_cds = {}\n",
    "    cluster_cds_ids = set([\"\", \"No protein ID\"]) # quick lookup for existing IDs\n",
    "    for feature in seq_record.features:\n",
    "        if feature.type == \"CDS\":\n",
    "            if (\"start_coord\" not in loci) or (feature.location.start >= loci[\"start_coord\"]-1 and feature.location.end <= loci[\"end_coord\"]):\n",
    "                if \"gene\" in feature.qualifiers:\n",
    "                    cluster_cds_ids.add(feature.qualifiers[\"gene\"][0])\n",
    "                    cluster_cds[feature.qualifiers[\"gene\"][0]] = feature\n",
    "                if \"protein_id\" in feature.qualifiers:\n",
    "                    cluster_cds_ids.add(feature.qualifiers[\"protein_id\"][0])\n",
    "                    cluster_cds[feature.qualifiers[\"protein_id\"][0]] = feature\n",
    "                if \"locus_tag\" in feature.qualifiers:\n",
    "                    cluster_cds_ids.add(feature.qualifiers[\"locus_tag\"][0])\n",
    "                    cluster_cds[feature.qualifiers[\"locus_tag\"][0]] = feature\n",
    "                \n",
    "    # fetch all CDS in MIBiG finalgbk\n",
    "    mibig_extra_genes = []\n",
    "    if int(bgc_id[3:]) < 1831:\n",
    "        with fetch_mibig_final_gbk(bgc_id) as mibig_gbk_handle:\n",
    "            mibig_seq_records = SeqIO.parse(mibig_gbk_handle, \"genbank\")\n",
    "            for mibig_seq_record in mibig_seq_records:\n",
    "                # look for gene information\n",
    "                match = re.search(\"(between(?P<start>\\d+)-(?P<end>\\d+)ntfrom){0,1}GenBankID(?P<acc>[A-Z0-9_\\.]+)\\.\", mibig_seq_record.annotations['comment'].replace(\" \", \"\").replace(\"\\n\", \"\"))\n",
    "                if match:\n",
    "                    gbk_acc_mibig = match.group(\"acc\").upper()\n",
    "                    if gbk_acc_mibig == gbk_acc:\n",
    "                        offset = 0\n",
    "                        if match.group(\"start\"):\n",
    "                            offset = int(match.group(\"start\")) - 1\n",
    "                        for feature in mibig_seq_record.features:\n",
    "                            if feature.type == \"CDS\":\n",
    "                                ids = []\n",
    "                                if \"gene\" in feature.qualifiers:\n",
    "                                    if feature.qualifiers[\"gene\"][0].upper() in [sanitise_gene_name(gid.upper()) for gid in cluster_cds_ids]:\n",
    "                                        continue\n",
    "                                    ids.append(feature.qualifiers[\"gene\"][0])\n",
    "                                    cluster_cds[feature.qualifiers[\"gene\"][0]] = feature\n",
    "                                if \"protein_id\" in feature.qualifiers:\n",
    "                                    if feature.qualifiers[\"protein_id\"][0].upper() in [sanitise_gene_name(gid.upper()) for gid in cluster_cds_ids]:\n",
    "                                        continue\n",
    "                                    ids.append(feature.qualifiers[\"protein_id\"][0])\n",
    "                                    cluster_cds[feature.qualifiers[\"protein_id\"][0]] = feature\n",
    "                                if \"locus_tag\" in feature.qualifiers:\n",
    "                                    if feature.qualifiers[\"locus_tag\"][0].upper() in [sanitise_gene_name(gid.upper()) for gid in cluster_cds_ids]:\n",
    "                                        continue\n",
    "                                    ids.append(feature.qualifiers[\"locus_tag\"][0])\n",
    "                                    cluster_cds[feature.qualifiers[\"locus_tag\"][0]] = feature\n",
    "                                cluster_cds_ids.update(set(ids))\n",
    "                                mibig_extra_genes.append(feature._shift(offset)) \n",
    "                                if bgc_id not in added_mibig_genes:\n",
    "                                    added_mibig_genes[bgc_id] = []\n",
    "                                added_mibig_genes[bgc_id].append(\"/\".join([gid for gid in ids]))\n",
    "                            \n",
    "    # add mibig extra genes\n",
    "    for feature in mibig_extra_genes:\n",
    "        if \"extra_genes\" not in annots:\n",
    "            annots[\"extra_genes\"] = []\n",
    "        if \"annotations\" not in annots:\n",
    "            annots[\"annotations\"] = []\n",
    "        gene_id = feature.qualifiers.get(\"locus_tag\", feature.qualifiers.get(\"protein_id\", feature.qualifiers.get(\"gene\", [None])))[0]\n",
    "        if gene_id != None:\n",
    "            extra_gene = {\n",
    "                \"id\": gene_id,\n",
    "                \"location\": {\n",
    "                    \"exons\": [{\"start\": location.start + 1, \"end\": location.end} for location in feature.location.parts],\n",
    "                    \"strand\": feature.location.strand\n",
    "                }\n",
    "            }\n",
    "            #if \"translation\" in feature.qualifiers:\n",
    "            #    extra_gene[\"translation\"] = feature.qualifiers[\"translation\"][0]\n",
    "            annots[\"extra_genes\"].append(extra_gene)\n",
    "            annot = {\n",
    "                \"id\": gene_id\n",
    "            }\n",
    "            if \"product\" in feature.qualifiers:\n",
    "                annot[\"product\"] = feature.qualifiers[\"product\"][0]\n",
    "            if \"note\" in feature.qualifiers:\n",
    "                annot[\"comments\"] = feature.qualifiers[\"note\"][0]\n",
    "            if len(annot.keys()) > 1:\n",
    "                annots[\"annotations\"].append(annot)\n",
    "\n",
    "    # check extra genes annotation\n",
    "    extra_genes = []\n",
    "    for i, extra_gene in enumerate(annots.get(\"extra_genes\", [])):\n",
    "        gene_id = extra_gene[\"id\"]\n",
    "        if gene_id.upper() not in [gid.upper() for gid in cluster_cds_ids] or gene_id.upper() in [gid.upper() for gid in \"/\".join(added_mibig_genes.get(bgc_id, \"\")).split(\"/\")]:\n",
    "            # check location, if outside the cluster, remove the locus information\n",
    "            if \"location\" in extra_gene:\n",
    "                exons = extra_gene[\"location\"].get(\"exons\", [])\n",
    "                if len(exons) > 0:\n",
    "                    cds_start = len(gbk_record.seq)\n",
    "                    cds_end = 0\n",
    "                    for exon in exons:\n",
    "                        if exon[\"start\"] < cds_start:\n",
    "                            cds_start = exon[\"start\"]\n",
    "                        if exon[\"end\"] > cds_end:\n",
    "                            cds_end = exon[\"end\"]\n",
    "                    if \"start_coord\" in loci and (cds_start < loci[\"start_coord\"] or cds_end > loci[\"end_coord\"]):\n",
    "                        extra_gene.pop(\"location\", None)\n",
    "                else:\n",
    "                    extra_gene.pop(\"location\", None)\n",
    "            extra_genes.append(extra_gene)\n",
    "            cluster_cds_ids.add(gene_id)\n",
    "        else:\n",
    "            if bgc_id not in removed_extra_genes:\n",
    "                removed_extra_genes[bgc_id] = []\n",
    "            removed_extra_genes[bgc_id].append(str(i))\n",
    "            \n",
    "    if len(extra_genes) > 0:\n",
    "        annots[\"extra_genes\"] = extra_genes\n",
    "    else:\n",
    "        annots.pop(\"extra_genes\", None)\n",
    "        \n",
    "    if (\"start_coord\" not in loci):\n",
    "        nt_length = len(gbk_record.seq)\n",
    "    else:\n",
    "        nt_length = loci[\"end_coord\"] - loci[\"start_coord\"]\n",
    "    gene_counts = len(cluster_cds.keys()) + len(extra_genes)\n",
    "    nt_length_and_cds_count[bgc_id] = (nt_length, gene_counts)\n",
    "            \n",
    "    # remove junk ids\n",
    "    cluster_cds_ids.discard(\"\")\n",
    "    cluster_cds_ids.discard(None)\n",
    "    \n",
    "    # check annotation, remove if no added info\n",
    "    gene_annots = []\n",
    "    for i, annot in enumerate(annots.get(\"annotations\", [])):\n",
    "        approve = False\n",
    "        found_id = None\n",
    "        for gid in cluster_cds_ids:\n",
    "            if annot.get(\"id\", \"\").upper() == gid.upper():\n",
    "                found_id = gid\n",
    "                annot[\"id\"] = gid\n",
    "                break\n",
    "            elif annot.get(\"name\", \"\").upper() == gid.upper():\n",
    "                found_id = gid\n",
    "                annot[\"name\"] = gid\n",
    "                break\n",
    "        ids = \"id='{}',name='{}'\".format(annot.get(\"id\", \"\"), annot.get(\"name\", \"\"))\n",
    "        if True:\n",
    "            if len(annot.get(\"name\", \"\")) < 1:\n",
    "                annot.pop(\"name\", None)\n",
    "            if len(annot.get(\"product\", \"\")) < 1:\n",
    "                annot.pop(\"product\", None)\n",
    "            if len(annot.get(\"mut_pheno\", \"\")) < 1:\n",
    "                annot.pop(\"mut_pheno\", None)\n",
    "            if len(annot.get(\"comments\", \"\")) < 1:\n",
    "                annot.pop(\"comments\", None)\n",
    "            g_functions = []\n",
    "            for g_function in annot.get(\"functions\", []):\n",
    "                if len(g_function.get(\"category\", \"\")) < 1:\n",
    "                    continue\n",
    "                elif len(g_function.get(\"evidence\", [])) < 1:\n",
    "                    continue\n",
    "                elif len(set(g_function[\"evidence\"]) - set([\"Sequence-based prediction\", \"Other in vivo study\", \"Heterologous expression\", \"Knock-out\", \"Activity assay\"])) > 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    g_functions.append(g_function)\n",
    "            if len(g_functions) > 0:\n",
    "                annot[\"functions\"] = g_functions\n",
    "            else:\n",
    "                annot.pop(\"functions\", None)\n",
    "            if len(annot.get(\"tailoring\", [])) < 1:\n",
    "                annot.pop(\"tailoring\", None)\n",
    "            if len(annot.get(\"publications\", [])) < 1:\n",
    "                annot.pop(\"publications\", None)\n",
    "                \n",
    "            if found_id == None:\n",
    "                annot = {} # uncomment to add to extra genes when not found                \n",
    "                #if len(annot.get(\"id\", \"\")) < 1:\n",
    "                #    if len(annot.get(\"name\", \"\")) < 1:\n",
    "                #        annot = {}\n",
    "                #    else:\n",
    "                #        annot[\"id\"] = annot[\"name\"]\n",
    "                #if len(annot.get(\"id\", \"\")) > 0:\n",
    "                #    if \"extra_genes\" not in annots:\n",
    "                #        annots[\"extra_genes\"] = []\n",
    "                #    annots[\"extra_genes\"].append({\"id\": annot[\"id\"]})\n",
    "                #    cluster_cds_ids.add(annot[\"id\"])\n",
    "                    \n",
    "            if len(annot.keys()) > 1: # 1 is the id\n",
    "                approve = True\n",
    "        if approve:\n",
    "            gene_annots.append(annot)\n",
    "        else:\n",
    "            if bgc_id not in removed_annotations:\n",
    "                removed_annotations[bgc_id] = []\n",
    "            removed_annotations[bgc_id].append(ids)\n",
    "    if len(gene_annots) > 0:\n",
    "        annots[\"annotations\"] = gene_annots\n",
    "    else:\n",
    "        annots.pop(\"annotations\", None)\n",
    "                \n",
    "    # check operons\n",
    "    operons = []\n",
    "    for i, operon in enumerate(annots.get(\"operons\", [])):\n",
    "        o_genes = set([gid.upper() for gid in operon.get(\"genes\", [])])\n",
    "        o_evidence = set(operon.get(\"evidence\", [])).intersection(set([\"Sequence-based prediction\", \"RACE\", \"ChIPseq\", \"RNAseq\"]))\n",
    "        if len(o_genes) > 0 and len(o_genes - cluster_cds_ids) < 1:\n",
    "            if len(o_evidence) > 0:\n",
    "                operons.append({\n",
    "                    \"genes\": list(o_genes),\n",
    "                    \"evidence\": list(o_evidence)\n",
    "                })\n",
    "                continue\n",
    "        if bgc_id not in removed_operons:\n",
    "            removed_operons[bgc_id] = []\n",
    "        removed_operons[bgc_id].append(str(i))\n",
    "        \n",
    "    if len(operons) > 0:\n",
    "        annots[\"operons\"] = operons\n",
    "    else:\n",
    "        annots.pop(\"operons\", None)\n",
    "\n",
    "    # try to fix ripp leader/follower sequences\n",
    "    if \"ripp\" in data[\"cluster\"]:\n",
    "        fixed_precursors = []\n",
    "        unfixable_precursors = []\n",
    "        for i, precursor in enumerate(data[\"cluster\"][\"ripp\"].get(\"precursor_genes\", [])):\n",
    "            if \"gene_id\" in precursor:\n",
    "                cds_feature = None\n",
    "                for gid in cluster_cds_ids:\n",
    "                    if precursor[\"gene_id\"].upper() == gid.upper():\n",
    "                        cds_feature = cluster_cds[gid]\n",
    "                        break\n",
    "                if cds_feature != None:\n",
    "                    if \"translation\" in cds_feature.qualifiers:\n",
    "                        aa_sequence = str(cds_feature.qualifiers[\"translation\"][0])\n",
    "                        leader_length = precursor.get(\"leader_sequence\", 0)\n",
    "                        follower_length = precursor.get(\"follower_sequence\", 0)\n",
    "                        if leader_length < 1 or leader_length > len(aa_sequence) or follower_length > len(aa_sequence):\n",
    "                            print(\"[ripp] Wrong leader/follower length information {}\".format(precursor[\"gene_id\"]))\n",
    "                            unfixable_precursors.append(i)\n",
    "                        elif len(precursor.get(\"core_sequence\", [])) < 1:\n",
    "                            print(\"[ripp] No core sequence {}\".format(precursor[\"gene_id\"]))\n",
    "                            unfixable_precursors.append(i)\n",
    "                        else:\n",
    "                            cores_checked = True\n",
    "                            for core in precursor[\"core_sequence\"]:\n",
    "                                if len(core) < 1:\n",
    "                                    cores_checked = False\n",
    "                                    break\n",
    "                                core_start = aa_sequence.find(core)\n",
    "                                if core_start < 0:\n",
    "                                    # can't find core\n",
    "                                    cores_checked = False\n",
    "                                    break\n",
    "                                elif core_start < leader_length or core_start + len(core) >= len(aa_sequence) - follower_length:\n",
    "                                    # core overlaps with leader/follower\n",
    "                                    cores_checked = False\n",
    "                                    break\n",
    "                            if cores_checked:\n",
    "                                precursor[\"leader_sequence\"] = aa_sequence[0:leader_length]\n",
    "                                if follower_length > 0:\n",
    "                                    precursor[\"follower_sequence\"] = aa_sequence[follower_length - 1:]\n",
    "                                else:\n",
    "                                    precursor.pop(\"follower_sequence\", None)\n",
    "                                fixed_precursors.append(i)\n",
    "                            else:\n",
    "                                print(\"[ripp] Wrong sequence information for {}\".format(precursor[\"gene_id\"]))\n",
    "                                unfixable_precursors.append(i)\n",
    "                    else:\n",
    "                        print(\"[ripp] Can't find AA sequence for {}\".format(precursor[\"gene_id\"]))\n",
    "                        unfixable_precursors.append(i)\n",
    "                else:\n",
    "                    print(\"[ripp] Can't find CDS feature for {}\".format(precursor[\"gene_id\"]))\n",
    "                    unfixable_precursors.append(i)\n",
    "                    \n",
    "        if len(fixed_precursors) > 0 and len(fixed_precursors) == len(data[\"cluster\"][\"ripp\"][\"precursor_genes\"]):\n",
    "            # remove the retired flag from ripp.leader/follower_sequence\n",
    "            print(\"[ripp] solved precursors {}\".format(bgc_id))\n",
    "            if bgc_id not in fixed:\n",
    "                fixed[bgc_id] = []\n",
    "            fixed[bgc_id].append(\"ripp.leader/follower_sequence\")\n",
    "        elif len(unfixable_precursors) > 0:\n",
    "            ripps_to_fix[bgc_id] = unfixable_precursors\n",
    "            # temporarily remove the precursors, and remove the retired flag from ripp.leader/follower_sequence\n",
    "            precursors = []\n",
    "            for i, precursor in enumerate(data[\"cluster\"][\"ripp\"][\"precursor_genes\"]):\n",
    "                if i not in unfixable_precursors:\n",
    "                    precursors.append(precursor)\n",
    "            if len(precursors) > 0:\n",
    "                data[\"cluster\"][\"ripp\"][\"precursor_genes\"] = precursors\n",
    "            else:\n",
    "                data[\"cluster\"][\"ripp\"].pop(\"precursor_genes\", None)\n",
    "            if bgc_id not in fixed:\n",
    "                fixed[bgc_id] = []\n",
    "            fixed[bgc_id].append(\"ripp.leader/follower_sequence\")            \n",
    "            \n",
    "        \n",
    "    # update gene ids in specific_infos\n",
    "    data = make_gene_id_match_reference(data, cluster_cds_ids)\n",
    "        \n",
    "    if len(annots.keys()) > 0:\n",
    "        data[\"cluster\"][\"genes\"] = annots\n",
    "    else:\n",
    "        data[\"cluster\"].pop(\"genes\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(data, validator, schema_props):\n",
    "    for error in sorted(validator.iter_errors(data), key=str):\n",
    "        try:\n",
    "            if error.path[-2] == \"evidence\" or error.path[-1] == \"evidence\":\n",
    "                if error.path[-3] == \"loci\" or error.path[-2] == \"loci\":\n",
    "                    continue\n",
    "            elif error.path[-1] == \"nr_iterations\":\n",
    "                continue\n",
    "            elif error.path[-1] == \"module_number\":\n",
    "                continue\n",
    "            elif error.path[-2] == \"proteinogenic\":\n",
    "                continue\n",
    "            elif error.path[-1] in [\"leader_sequence\", \"follower_sequence\"]:\n",
    "                continue\n",
    "            elif error.path[-1] == \"organism\":\n",
    "                if data[\"cluster\"][\"mibig_accession\"] in no_gbk:\n",
    "                    continue\n",
    "            elif error.path[-1] == \"subcluster\" and error.path[-3] == \"chem_moieties\":\n",
    "                continue\n",
    "        except:\n",
    "            pass\n",
    "        print(error.message)\n",
    "        #sys.exit(0)\n",
    "    this_file_props = count_props(data, \"\", {})\n",
    "    for prop in this_file_props:\n",
    "        if prop not in schema_props.keys():\n",
    "            print(prop)\n",
    "            sys.exit(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"../../preprocessed/p6-json/\"\n",
    "output_folder = \"../../preprocessed/p7-json/\"\n",
    "\n",
    "if not path.exists(output_folder):\n",
    "    makedirs(output_folder)\n",
    "\n",
    "validator = None\n",
    "schema_props = {}\n",
    "with open(\"../../inputs/mibig_schema_phase_6.json\") as json_file:\n",
    "    schema_obj = json.load(json_file)\n",
    "    validator = Draft7Validator(schema_obj)\n",
    "    schema_props = fetch_props_new_schema(schema_obj, \"\", {})\n",
    "\n",
    "\n",
    "for json_path in sorted(glob.glob(path.join(input_path, \"BGC*.json\"))):\n",
    "    with open(json_path, \"r\") as json_file:\n",
    "        bgc_id = path.basename(json_path).split(\".\")[0]\n",
    "        data = json.load(json_file)\n",
    "        print(\"Scanning {}\".format(bgc_id))\n",
    "        check_gene_annotations(data)\n",
    "        validate_data(data, validator, schema_props)\n",
    "        with open(path.join(output_folder, \"{}.json\".format(bgc_id)), \"w\") as o:\n",
    "            o.write(json.dumps(data, indent=4, separators=(',', ': '), sort_keys=True))\n",
    "            \n",
    "print(\"All data fetched!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retired_bgcs = {}\n",
    "todo_list = {}\n",
    "\n",
    "with open(\"../../preprocessed/reports/p7-removed_extra_genes.tsv\", \"w\") as o:\n",
    "    o.write(\"bgc_id\\tgene_ids\\n\")\n",
    "    for bgc_id in removed_extra_genes:\n",
    "        o.write(\"{}\\t{}\\n\".format(bgc_id, \",\".join(removed_extra_genes[bgc_id])))\n",
    "        # uncommented: we don't need to retire BGCs because we trimmed out extra_genes\n",
    "        #if bgc_id not in retired_bgcs:\n",
    "        #    retired_bgcs[bgc_id] = set()\n",
    "        #retired_bgcs[bgc_id].add(\"extra_genes_removed\")\n",
    "    \n",
    "with open(\"../../preprocessed/reports/p7-removed_annotations.tsv\", \"w\") as o:\n",
    "    o.write(\"bgc_id\\tgene_ids\\n\")\n",
    "    for bgc_id in removed_annotations:\n",
    "        o.write(\"{}\\t{}\\n\".format(bgc_id, \";\".join(removed_annotations[bgc_id])))\n",
    "        if bgc_id not in todo_list:\n",
    "            todo_list[bgc_id] = {}\n",
    "        todo_list[bgc_id][\"annotations_removed\"] = removed_annotations[bgc_id]\n",
    "    \n",
    "with open(\"../../preprocessed/reports/p7-removed_operons.tsv\", \"w\") as o:\n",
    "    o.write(\"bgc_id\\tindexes\\n\")\n",
    "    for bgc_id in removed_operons:\n",
    "        o.write(\"{}\\t{}\\n\".format(bgc_id, \",\".join(removed_operons[bgc_id])))\n",
    "        if bgc_id not in todo_list:\n",
    "            todo_list[bgc_id] = {}\n",
    "        todo_list[bgc_id][\"operons_removed\"] = removed_operons[bgc_id]\n",
    "\n",
    "with open(\"../../preprocessed/reports/p7-no_gbk.tsv\", \"w\") as o:\n",
    "    o.write(\"bgc_id\\taccession\\n\")\n",
    "    for bgc_id in no_gbk:\n",
    "        o.write(\"{}\\t{}\\n\".format(bgc_id, no_gbk[bgc_id]))\n",
    "        if bgc_id not in retired_bgcs:\n",
    "            retired_bgcs[bgc_id] = set()\n",
    "        retired_bgcs[bgc_id].add(\"no_gbk\")\n",
    "    \n",
    "with open(\"../../preprocessed/reports/p7-no_taxonomy.tsv\", \"w\") as o:\n",
    "    o.write(\"bgc_id\\torganism\\n\")\n",
    "    for bgc_id in no_taxid:\n",
    "        o.write(\"{}\\t{}\\n\".format(bgc_id, no_taxid[bgc_id]))\n",
    "        if bgc_id not in retired_bgcs:\n",
    "            retired_bgcs[bgc_id] = set()\n",
    "        retired_bgcs[bgc_id].add(\"no_taxonomy\")\n",
    "        \n",
    "        \n",
    "with open(\"../../preprocessed/reports/p7-retired_list.tsv\", \"w\") as o:\n",
    "    for bgc_id in retired_bgcs:\n",
    "        o.write(\"{}\\t{}\\n\".format(bgc_id, \";\".join(retired_bgcs[bgc_id])))\n",
    "    \n",
    "with open(\"../../preprocessed/reports/p7-added_mibig_genes.tsv\", \"w\") as o:\n",
    "    o.write(\"bgc_id\\tgenes\\n\")\n",
    "    for bgc_id in added_mibig_genes:\n",
    "        o.write(\"{}\\t{}\\n\".format(bgc_id, \",\".join(added_mibig_genes[bgc_id])))\n",
    "\n",
    "with open(\"../../preprocessed/reports/p7-ripps_precursors_to_fix.tsv\", \"w\") as o:\n",
    "    for bgc_id in ripps_to_fix:\n",
    "        o.write(\"{}\\t{}\\n\".format(bgc_id, \",\".join([str(idx) for idx in ripps_to_fix[bgc_id]])))\n",
    "        if bgc_id not in todo_list:\n",
    "            todo_list[bgc_id] = {}\n",
    "        todo_list[bgc_id][\"ripp_precursors\"] = ripps_to_fix[bgc_id]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### update fixed list ###\n",
    "with open(\"../../preprocessed/reports/p7-fixed_list.tsv\", \"w\") as o:\n",
    "    for bgc_id in fixed:\n",
    "        o.write(\"{}\\t{}\\n\".format(bgc_id, \";\".join(fixed[bgc_id])))\n",
    "        \n",
    "with open(\"../../preprocessed/reports/p7-todo_list.tsv\", \"w\") as o:\n",
    "    for bgc_id in todo_list:\n",
    "        for issue in todo_list[bgc_id]:\n",
    "            o.write(\"{}\\t{}\\t{}\\n\".format(bgc_id, issue, \";\".join([str(idx) for idx in todo_list[bgc_id][issue]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../../preprocessed/p7-gene_counts_and_loci_length.tsv\", \"w\") as o:\n",
    "    o.write(\"bgc_id\\tnt_length\\tcds_count\\n\")\n",
    "    for bgc_id in nt_length_and_cds_count:\n",
    "        nt_length, gene_counts = nt_length_and_cds_count[bgc_id]\n",
    "        o.write(\"{}\\t{}\\t{}\\n\".format(bgc_id, nt_length, gene_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
